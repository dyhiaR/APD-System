# -*- coding: utf-8 -*-
"""ReLu_IoHT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xnAgW8YLmsJifNXYDFYp7sgVHbQC7lzW
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

#Load training and test files
train_data = pd.read_csv("/content/540-ws-training_processed.csv")
test_data = pd.read_csv("/content/540-ws-testing_processed.csv")
gen_data = pd.read_csv("/content/544-ws-testing_processed.csv")


# Show data preview
print("Training data :")
print(train_data.head())
print("\ntest data :")
print(test_data.head())
print("\nGeneralization data :")
print(gen_data.head())

train_data= train_data[['5minute_intervals_timestamp','cbg']]
test_data= test_data[['5minute_intervals_timestamp','cbg']]
gen_data= gen_data[['5minute_intervals_timestamp','cbg']]
print("Training data :")
print(train_data.head())
print("\nTest Data :")
print(test_data.head())
print("\nGeneralization data :")
print(gen_data.head())

print(train_data.isnull().sum())
print(test_data.isnull().sum())
print(gen_data.isnull().sum())

train_data = train_data.dropna()
test_data = test_data.dropna()
gen_data = gen_data.dropna()

scaler = MinMaxScaler()

# Normalize glucose levels in both sets
train_data["cbg"] = scaler.fit_transform(train_data[["cbg"]])
test_data["cbg"] = scaler.transform(test_data[["cbg"]])
gen_data["cbg"] = scaler.transform(gen_data[["cbg"]])

def create_sequences(data, sequence_length):
    X, y = [], []
    for i in range(len(data) - sequence_length):
        X.append(data[i:i + sequence_length])
        y.append(data[i + sequence_length])
    return np.array(X), np.array(y)

sequence_length = 10

# Create the sequences for training
X_train, y_train = create_sequences(train_data["cbg"].values, sequence_length)

# Create the sequences for the test
X_test, y_test = create_sequences(test_data["cbg"].values, sequence_length)

# Create the sequences for generalization
X_gen, y_gen = create_sequences(gen_data["cbg"].values, sequence_length)

# Reshape the data
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))
X_gen = X_gen.reshape((X_gen.shape[0], X_gen.shape[1], 1))

print(f"Shape of training data : {X_train.shape}")
print(f"Shape of test data {X_test.shape}")
print(f"Shape of generalization data : {X_gen.shape}")

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, Input

# Create model
model = Sequential([
    Input(shape=(X_train.shape[1], X_train.shape[2])),
    SimpleRNN(10, activation='relu', return_sequences=False, return_state=False),  # Activation: 'relu'
    Dense(1)  # Single output: Glucose level predictionse
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Show a summary of the model
model.summary()

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=20,
    batch_size=32,
    verbose=1
)

loss, mae = model.evaluate(X_test, y_test)
print(f"Mean Absolute Error on test data: {mae}")

loss1, mae1 = model.evaluate(X_gen, y_gen)
print(f"Mean Absolute Error on data generalisation : {mae1}")

import matplotlib.pyplot as plt
import numpy as np

# Plot loss curves during training
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Loss - Training')
plt.plot(history.history['val_loss'], label='loss - Validation')
plt.title('Loss Curve')
plt.xlabel('Epochs')
plt.ylabel('MSE (Mean Squared Error)')
plt.legend()
plt.show()

# Plot MAE (Mean Absolute Error) curves during training
plt.figure(figsize=(10, 5))
plt.plot(history.history['mae'], label='MAE - Training')
plt.plot(history.history['val_mae'], label='MAE - Validation')
plt.title('Mean Absolute Error Curve')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()
plt.show()

# Compare actual and predicted values ​​on test data
y_test_pred = model.predict(X_test)

plt.figure(figsize=(10, 5))
plt.plot(y_test, label='Real Values ​​- Test')
plt.plot(y_test_pred, label='Predicted Values ​​- Test')
plt.ylim(min(y_test) * 0.95, max(y_test) * 1.05)
plt.title('Comparison of Actual and Predicted Values ​​(Test)')
plt.xlabel('Exemples')
plt.ylabel(' Glucose Level')
plt.legend()
plt.show()

# Compare actual and predicted values ​​on generalization data
y_gen_pred = model.predict(X_gen)

plt.figure(figsize=(10, 5))
plt.plot(y_gen, label='Real Values - Generalisation')
plt.plot(y_gen_pred, label='Predicted Values - Generalisation')
plt.ylim(min(y_gen) * 0.95, max(y_gen) * 1.05)
plt.title('Comparison of Actual and Predicted Values (Generalisation)')
plt.xlabel('Exemples')
plt.ylabel('Glucose Level')
plt.legend()
plt.show()

# Save model weights
model.save_weights('/content/rnn_weights.weights.h5')
print("Saved model weights.")

from sklearn.metrics import mean_absolute_error, mean_squared_error
import numpy as np

def evaluate_future_predictions(model, X_test, y_test, steps=6, num_samples=40):
    """
    Tests the model on a subset of num_samples sequences and predicts blood glucose levels over 30 minutes.

    :param model: The trained RNN model.
    :param X_test: Test sequences (shape: nb_samples, sequence_length, 1).
    :param y_test: True test values corresponding to the sequences.
    :param steps: Number of prediction steps (6 * 5 min = 30 min).
    :param num_samples: Number of samples to test (default: 20).
    :return: List of predictions and evaluation metrics.
    """
    # Randomly select 20 indices from X_test
    indices = np.random.choice(len(X_test), num_samples, replace=False)

    all_predictions = []
    true_values = []  # Stores true values at 30 min

    for i in indices:
        current_sequence = X_test[i].reshape(1, sequence_length, 1)  # Reshape for model input
        future_predictions = []  # Stores the next 6 predictions

        for _ in range(steps):
            next_glucose = model.predict(current_sequence, verbose=0)[0, 0]  # Single prediction
            future_predictions.append(next_glucose)

            # Update the input sequence
            current_sequence = np.roll(current_sequence, -1, axis=1)  # Shift left
            current_sequence[0, -1, 0] = next_glucose  # Add the new value

        all_predictions.append(future_predictions[-1])  # Last prediction = 30 min
        true_values.append(y_test[i + steps - 1])  # True value after 30 min

    # Convert to numpy arrays
    all_predictions = np.array(all_predictions)
    true_values = np.array(true_values)

    # Compute evaluation metrics
    mae = mean_absolute_error(true_values, all_predictions)
    mse = mean_squared_error(true_values, all_predictions)

    return all_predictions, true_values, mae, mse

# Run evaluation on 20 test sequences
predictions_30min, true_30min, mae, mse = evaluate_future_predictions(model, X_test, y_test)

# Display results
print(f"Mean Absolute Error (MAE) after 30 minutes: {mae:.4f}")
print(f"Mean Squared Error (MSE) after 30 minutes: {mse:.4f}")

# Example of displaying 5 predictions compared to true values
for i in range(5):
    print(f"True value after 30 min: {true_30min[i]:.2f} | Prediction: {predictions_30min[i]:.2f}")

import matplotlib.pyplot as plt

def plot_predictions(true_values, predicted_values):
    """
    Displays a graph comparing the true values and the model predictions after 30 minutes.

    :param true_values: True glucose values after 30 minutes.
    :param predicted_values: Model predictions after 30 minutes.
    """
    plt.figure(figsize=(8, 5))

    # Plot the true values and predictions
    plt.plot(true_values, label="True values", marker='o', linestyle='dashed')
    plt.plot(predicted_values, label="Model predictions", marker='x', linestyle='dashed')

    # Add labels and a title
    plt.xlabel("Test sequence (samples)")
    plt.ylabel("Glucose after 30 min (mg/dL)")
    plt.title("Comparison of True Values vs Model Predictions After 30 Minutes")
    plt.legend()
    plt.grid(True)

    # Display the graph
    plt.show()

# Display the results
plot_predictions(true_30min, predictions_30min)

!pip install tenseal

import tenseal as ts
import numpy as np
import tensorflow as tf

def relu_approximation_homomorphic(encrypted_x):
    """
    Homomorphic approximation of ReLU(x)
    """
    # Use a soft ReLU approximation
    zero = encrypted_x.mul(0)
    mask = encrypted_x.sub(zero)  # Create a mask for positive values
    result = mask
    return result

def create_context():
    poly_mod_degree = 32768
    coeff_mod_bit_sizes = [40, 22, 22, 40]
    context = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, coeff_mod_bit_sizes)
    context.global_scale = 2**14
    context.generate_galois_keys()
    return context

def load_model_parameters():
    # Use Input layer to avoid warnings
    inputs = tf.keras.Input(shape=(10, 1))
    x = tf.keras.layers.SimpleRNN(10, activation='relu')(inputs)
    outputs = tf.keras.layers.Dense(1)(x)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    # Load the saved weights
    model.load_weights("/content/rnn_weights.weights.h5")

    # Extract the weights
    rnn_kernel, rnn_recurrent_kernel, rnn_bias = model.layers[1].get_weights()
    dense_weights, dense_bias = model.layers[2].get_weights()

    return rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias

def normalize_sequence(sequence):
    sequence = np.array(sequence, dtype=np.float32)
    min_val = sequence.min()
    max_val = sequence.max()
    normalized_sequence = (sequence - min_val) / (max_val - min_val)
    return normalized_sequence, min_val, max_val

def inference_homomorphic(context, rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, sequence):
    h_t = ts.ckks_vector(context, np.zeros(10))

    for t in range(10):
        x_t = ts.ckks_vector(context, [sequence[t]])

        # Homomorphic computation: z_t = W_h * x_t + U_h * h_t + b_h
        z_t = x_t.matmul(rnn_kernel)
        z_t = z_t.add(h_t.matmul(rnn_recurrent_kernel))
        z_t = z_t.add(rnn_bias)

        # Approximation of ReLU(z_t) to obtain h_t
        h_t = relu_approximation_homomorphic(z_t)

    # Compute the final output: y = Dense(h_t)
    y_t = h_t.matmul(dense_weights).add(dense_bias)

    return y_t

def test_precision():
    # Load the model parameters
    rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias = load_model_parameters()

    # Input sequence
    raw_sequence = [254, 250, 249, 247, 242, 235, 229, 224, 220, 217]

    # Normalize the sequence
    normalized_sequence, min_val, max_val = normalize_sequence(raw_sequence)

    # Plain inference
    def inference_plain(rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, sequence):
        h_t = np.zeros(10, dtype=np.float32)
        for t in range(10):
            x_t = np.array([sequence[t]], dtype=np.float32)
            z_t = np.dot(rnn_kernel.T, x_t) + np.dot(rnn_recurrent_kernel.T, h_t) + rnn_bias
            h_t = np.maximum(z_t, 0)  # Real ReLU function
        y_t = np.dot(h_t, dense_weights) + dense_bias
        return y_t

    # Plain prediction
    plain_output = inference_plain(rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, normalized_sequence)
    plain_output_denormalized = plain_output * (max_val - min_val) + min_val

    # Homomorphic inference
    context = create_context()
    encrypted_output = inference_homomorphic(context, rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, normalized_sequence)
    encrypted_output_denormalized = np.array(encrypted_output.decrypt()) * (max_val - min_val) + min_val

    # Compute the absolute error
    error = np.abs(plain_output_denormalized - encrypted_output_denormalized)

    # Compute the MAE (Mean Absolute Error)
    mae = np.mean(np.abs(plain_output_denormalized - encrypted_output_denormalized))

    print(f"Plain prediction: {plain_output_denormalized}")
    print(f"Homomorphic prediction: {encrypted_output_denormalized}")
    print(f"Absolute error: {error}")
    print(f"Mean Absolute Error (MAE): {mae}")

# Run the test
test_precision()

import numpy as np
import tenseal as ts
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error


# Function to create the encryption context
def create_context():
    poly_mod_degree = 32768
    coeff_mod_bit_sizes = [40, 22, 22, 40]
    context = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, coeff_mod_bit_sizes)
    context.global_scale = 2**14
    context.generate_galois_keys()
    print("Encryption context created.")
    return context

# Function to load the model parameters
def load_model_parameters():
    inputs = tf.keras.Input(shape=(10, 1))
    x = tf.keras.layers.SimpleRNN(10, activation='relu')(inputs)
    outputs = tf.keras.layers.Dense(1)(x)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    # Load the saved weights
    model.load_weights("/content/rnn_weights.weights.h5")

    # Extract the weights
    rnn_kernel, rnn_recurrent_kernel, rnn_bias = model.layers[1].get_weights()
    dense_weights, dense_bias = model.layers[2].get_weights()

    print("Model parameters loaded.")
    return rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias

# Function to normalize the sequence
def normalize_sequence(sequence):
    sequence = np.array(sequence, dtype=np.float32)
    min_val = sequence.min()
    max_val = sequence.max()
    normalized_sequence = (sequence - min_val) / (max_val - min_val)
    print(f"Normalized sequence: {normalized_sequence}")
    return normalized_sequence, min_val, max_val

# Plain inference function
def inference_plain(rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, sequence):
    h_t = np.zeros(10, dtype=np.float32)
    for t in range(10):
        x_t = np.array([sequence[t]], dtype=np.float32)
        z_t = np.dot(rnn_kernel.T, x_t) + np.dot(rnn_recurrent_kernel.T, h_t) + rnn_bias
        h_t = np.maximum(z_t, 0)  # Real ReLU function
    y_t = np.dot(h_t, dense_weights) + dense_bias
    return y_t

# Homomorphic ReLU approximation function
def relu_approximation_homomorphic(encrypted_x):
    zero = encrypted_x.mul(0)
    mask = encrypted_x.sub(zero)  # Create a mask for positive values
    result = mask
    return result

# Homomorphic inference function
def inference_homomorphic(context, rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, sequence):
    h_t = ts.ckks_vector(context, np.zeros(10))

    for t in range(10):
        x_t = ts.ckks_vector(context, [sequence[t]])

        # Homomorphic computation: z_t = W_h * x_t + U_h * h_t + b_h
        z_t = x_t.matmul(rnn_kernel)
        z_t = z_t.add(h_t.matmul(rnn_recurrent_kernel))
        z_t = z_t.add(rnn_bias)

        # Approximation of ReLU(z_t) to obtain h_t
        h_t = relu_approximation_homomorphic(z_t)

    # Compute the final output: y = Dense(h_t)
    y_t = h_t.matmul(dense_weights).add(dense_bias)
    return y_t

# Function to plot a single diagram of the results
def plot_results_single(raw_sequences, plain_outputs, encrypted_outputs, errors):
    num_sequences = len(raw_sequences)
    sequence_indices = range(num_sequences)  # Sequence indices

    # Convert data to scalars if they are arrays
    plain_outputs = np.array(plain_outputs).flatten()
    encrypted_outputs = np.array(encrypted_outputs).flatten()
    errors = np.array(errors).flatten()

    # Configure the figure
    plt.figure(figsize=(12, 8))

    # Plot plaintext predictions
    plt.plot(sequence_indices, plain_outputs, 'go-', label="Plain predictions (denormalized)")

    # Plot homomorphic predictions
    plt.plot(sequence_indices, encrypted_outputs, 'ro-', label="Homomorphic predictions (denormalized)")

    # Plot absolute errors
    plt.bar(sequence_indices, errors, alpha=0.5, color="orange", label="Absolute error")

    # Add custom labels and title
    plt.title("Predictions and Errors for All Sequences", fontsize=16)
    plt.xlabel("Sequence Index", fontsize=14)
    plt.ylabel("Values / Errors", fontsize=14)
    plt.xticks(sequence_indices, [f"Seq {i+1}" for i in sequence_indices], fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(axis="y", linestyle="--", alpha=0.7)

    # Display the graph
    plt.tight_layout()
    plt.show()

# Modified test function to include outputs and errors in a single plot
def test_precision_multiple_sequences_with_single_plot():
    # Load model parameters
    rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias = load_model_parameters()

    # Set of input sequences
    raw_sequences = [
        [254, 250, 249, 247, 242, 235, 229, 224, 220, 217],
        [215, 212, 209, 205, 208, 210, 206, 201, 198, 195],
        [192, 191, 188, 185, 184, 182, 180, 179, 177, 176],
        [174, 170, 166, 162, 163, 163, 162, 163, 167, 169],
        [168, 167, 166, 165, 165, 164, 166, 164, 161, 158],
        [152, 147, 160, 169, 179, 184, 198, 200, 203, 204],
        [206, 207, 207, 200, 196, 193, 190, 186, 178, 174],
        [170, 169, 173, 166, 159, 150, 143, 142, 142, 147],
        [148, 149, 149, 146, 144, 141, 133, 130, 132, 127],
        [119, 115, 113, 110, 106, 104, 103, 101, 98, 97]
    ]

    # Initialize lists for outputs and errors
    plain_outputs = []
    encrypted_outputs = []
    error_list = []

    # Create the encryption context
    context = create_context()

    for raw_sequence in raw_sequences:
        print(f"\nProcessing sequence: {raw_sequence}")

        # Normalize the sequence
        normalized_sequence, min_val, max_val = normalize_sequence(raw_sequence)

        # Plaintext prediction
        plain_output = inference_plain(rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, normalized_sequence)
        plain_output_denormalized = plain_output.flatten()[0] * (max_val - min_val) + min_val
        print(f"Denormalized plaintext prediction: {plain_output_denormalized}")

        # Homomorphic prediction
        encrypted_output = inference_homomorphic(context, rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, normalized_sequence)
        encrypted_output_denormalized = np.array(encrypted_output.decrypt())[0] * (max_val - min_val) + min_val
        print(f"Denormalized homomorphic prediction: {encrypted_output_denormalized}")

        # Calculate the absolute error
        error = np.abs(plain_output_denormalized - encrypted_output_denormalized)
        print(f"Absolute error: {error}")

        # Append the results
        plain_outputs.append(plain_output_denormalized)
        encrypted_outputs.append(encrypted_output_denormalized)
        error_list.append(error)

    # Compute the MAE and MSE metrics
    mae = mean_absolute_error(plain_outputs, encrypted_outputs)
    mse = mean_squared_error(plain_outputs, encrypted_outputs)

    print(f"\nMean Absolute Error (MAE): {mae:.4f}")
    print(f"Mean Squared Error (MSE): {mse:.4f}")

    # Plot the overall results
    plot_results_single(raw_sequences, plain_outputs, encrypted_outputs, error_list)

# To execute the test, simply call the function:
test_precision_multiple_sequences_with_single_plot()

import numpy as np
import tenseal as ts
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error


# Function to create the encryption context
def create_context():
    poly_mod_degree = 32768
    coeff_mod_bit_sizes = [40, 22, 22, 40]
    context = ts.context(ts.SCHEME_TYPE.CKKS, poly_mod_degree, coeff_mod_bit_sizes)
    context.global_scale = 2**14
    context.generate_galois_keys()
    print("Encryption context created.")
    return context

# Function to load the model parameters
def load_model_parameters():
    inputs = tf.keras.Input(shape=(10, 1))
    x = tf.keras.layers.SimpleRNN(10, activation='relu')(inputs)
    outputs = tf.keras.layers.Dense(1)(x)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)

    # Load the saved weights
    model.load_weights("/content/rnn_weights.weights.h5")

    # Extract the weights
    rnn_kernel, rnn_recurrent_kernel, rnn_bias = model.layers[1].get_weights()
    dense_weights, dense_bias = model.layers[2].get_weights()

    print("Model parameters loaded.")
    return rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias

# Function to normalize the sequence
def normalize_sequence(sequence):
    sequence = np.array(sequence, dtype=np.float32)
    min_val = sequence.min()
    max_val = sequence.max()
    normalized_sequence = (sequence - min_val) / (max_val - min_val)
    print(f"Normalized sequence: {normalized_sequence}")
    return normalized_sequence, min_val, max_val

# Plain inference function
def inference_plain(rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, sequence):
    h_t = np.zeros(10, dtype=np.float32)
    for t in range(10):
        x_t = np.array([sequence[t]], dtype=np.float32)
        z_t = np.dot(rnn_kernel.T, x_t) + np.dot(rnn_recurrent_kernel.T, h_t) + rnn_bias
        h_t = np.maximum(z_t, 0)  # Real ReLU function
    y_t = np.dot(h_t, dense_weights) + dense_bias
    return y_t

# Homomorphic ReLU approximation function
def relu_approximation_homomorphic(encrypted_x):
    zero = encrypted_x.mul(0)
    mask = encrypted_x.sub(zero)  # Create a mask for positive values
    result = mask
    return result

# Homomorphic inference function
def inference_homomorphic(context, rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, sequence):
    h_t = ts.ckks_vector(context, np.zeros(10))

    for t in range(10):
        x_t = ts.ckks_vector(context, [sequence[t]])

        # Homomorphic computation: z_t = W_h * x_t + U_h * h_t + b_h
        z_t = x_t.matmul(rnn_kernel)
        z_t = z_t.add(h_t.matmul(rnn_recurrent_kernel))
        z_t = z_t.add(rnn_bias)

        # Approximation of ReLU(z_t) to obtain h_t
        h_t = relu_approximation_homomorphic(z_t)

    # Compute the final output: y = Dense(h_t)
    y_t = h_t.matmul(dense_weights).add(dense_bias)
    return y_t

# Function to plot a single diagram of the results
def plot_results_single(raw_sequences, plain_outputs, encrypted_outputs, error_percentages):
    num_sequences = len(raw_sequences)
    sequence_indices = range(num_sequences)  # Sequence indices

    # Convert data to scalars if they are arrays
    plain_outputs = np.array(plain_outputs).flatten()
    encrypted_outputs = np.array(encrypted_outputs).flatten()
    error_percentages = np.array(error_percentages).flatten()

    # Configure the figure
    plt.figure(figsize=(12, 8))

    # Plot plaintext predictions
    plt.plot(sequence_indices, plain_outputs, 'go-', label="Plain predictions (denormalized)")

    # Plot homomorphic predictions
    plt.plot(sequence_indices, encrypted_outputs, 'ro-', label="Homomorphic predictions (denormalized)")

    # Plot percentage errors
    plt.bar(sequence_indices, error_percentages, alpha=0.5, color="orange", label="Error (%)")

    # Add custom labels and title
    plt.title("Predictions and Percentage Errors for All Sequences", fontsize=16)
    plt.xlabel("Sequence Index", fontsize=14)
    plt.ylabel("Values / Error (%)", fontsize=14)
    plt.xticks(sequence_indices, [f"Seq {i+1}" for i in sequence_indices], fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(axis="y", linestyle="--", alpha=0.7)

    # Display the graph
    plt.tight_layout()
    plt.show()

# Modified test function to include outputs and errors in a single plot
def test_precision_multiple_sequences_with_single_plot():
    # Load model parameters
    rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias = load_model_parameters()

    # Set of input sequences
    raw_sequences = [
        [254, 250, 249, 247, 242, 235, 229, 224, 220, 217],
        [215, 212, 209, 205, 208, 210, 206, 201, 198, 195],
        [192, 191, 188, 185, 184, 182, 180, 179, 177, 176],
        [174, 170, 166, 162, 163, 163, 162, 163, 167, 169],
        [168, 167, 166, 165, 165, 164, 166, 164, 161, 158],
        [152, 147, 160, 169, 179, 184, 198, 200, 203, 204],
        [206, 207, 207, 200, 196, 193, 190, 186, 178, 174],
        [170, 169, 173, 166, 159, 150, 143, 142, 142, 147],
        [148, 149, 149, 146, 144, 141, 133, 130, 132, 127],
        [119, 115, 113, 110, 106, 104, 103, 101, 98, 97],
        [99, 98, 97, 95, 93,93, 90, 89, 90, 90],
        [90, 89, 86, 85, 88, 93, 99, 107, 118, 130],
        [142, 152, 160, 166, 172, 173, 172, 171, 171, 168],
        [164, 162, 162, 155, 147, 145, 137, 132, 135, 132],
        [127, 123, 113, 105, 102, 95, 91, 86, 83, 80],
        [78, 78, 57, 56, 60, 68, 75, 85, 96, 103],
        [112, 116, 117, 116, 112, 108, 106, 105, 104, 102],
        [101, 99, 97, 95, 95, 97, 99, 97, 93, 94],
        [98, 101, 108, 112, 115,120, 120, 117, 113, 109],
        [105, 103, 99, 91, 87, 90, 89, 90, 94, 96]
    ]

    # Initialize lists for outputs and error percentages
    plain_outputs = []
    encrypted_outputs = []
    error_percentage_list = []

    # Create the encryption context
    context = create_context()

    for raw_sequence in raw_sequences:
        print(f"\nProcessing sequence: {raw_sequence}")

        # Normalize the sequence
        normalized_sequence, min_val, max_val = normalize_sequence(raw_sequence)

        # Plaintext prediction
        plain_output = inference_plain(rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, normalized_sequence)
        plain_output_denormalized = plain_output.flatten()[0] * (max_val - min_val) + min_val
        print(f"Denormalized plaintext prediction: {plain_output_denormalized}")

        # Homomorphic prediction
        encrypted_output = inference_homomorphic(context, rnn_kernel, rnn_recurrent_kernel, rnn_bias, dense_weights, dense_bias, normalized_sequence)
        encrypted_output_denormalized = np.array(encrypted_output.decrypt())[0] * (max_val - min_val) + min_val
        print(f"Denormalized homomorphic prediction: {encrypted_output_denormalized}")

        # Calculate the absolute error and convert it to a percentage relative to the plaintext prediction
        absolute_error = np.abs(plain_output_denormalized - encrypted_output_denormalized)
        error_percentage = (absolute_error / plain_output_denormalized) * 100
        print(f"Absolute error: {absolute_error}")
        print(f"Error percentage: {error_percentage:.2f}%")

        # Append the results
        plain_outputs.append(plain_output_denormalized)
        encrypted_outputs.append(encrypted_output_denormalized)
        error_percentage_list.append(error_percentage)

    # Compute the MAE and MSE metrics
    mae = mean_absolute_error(plain_outputs, encrypted_outputs)
    mse = mean_squared_error(plain_outputs, encrypted_outputs)

    print(f"\nMean Absolute Error (MAE): {mae:.4f}")
    print(f"Mean Squared Error (MSE): {mse:.4f}")

    # Plot the overall results with error percentages
    plot_results_single(raw_sequences, plain_outputs, encrypted_outputs, error_percentage_list)

# To execute the test, simply call the function:
test_precision_multiple_sequences_with_single_plot()